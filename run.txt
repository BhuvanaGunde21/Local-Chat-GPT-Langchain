To get started with Local Multimodal AI Chat, clone the repository and follow these simple steps:
 Go to terminal :
Upgrade pip: pip install --upgrade pip

Install Requirements: pip install -r requirements.txt

Note: in requirements_with_versions.txt I saved the versions of the requirements while creating this project, and in pip_freeze.txt is a complete freeze of the packages in the environment I used. So if you encounter errors due to newer versions, you might want to consider using one of those requirements files, or at least the versions for the packages which make problems.

Windows Users: The installation might differ a bit for you, if you encounter errors you can't solve, please open an Issue here on github.

Setting Up Local Models: Download the models you want to implement. Here is the llava model I used for image chat (ggml-model-q5_k.gguf and mmproj-model-f16.gguf). And the quantized mistral model form TheBloke.

Customize config file: Check the config file and change accordingly to the models you downloaded.

Enter command in terminal: streamlit run app.py
